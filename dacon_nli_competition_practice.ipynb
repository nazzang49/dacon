{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazzang49/dacon/blob/main/dacon_nli_competition_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dacon NLI Competition Practice**\n",
        "https://dacon.io/competitions/official/235875/overview/description"
      ],
      "metadata": {
        "id": "3p7Qc6FodvBn"
      },
      "id": "3p7Qc6FodvBn"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gluonnlp transformers kobert-transformers mxnet"
      ],
      "metadata": {
        "id": "Fj63NMddaWS7"
      },
      "id": "Fj63NMddaWS7",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/SKTBrain/KoBERT.git"
      ],
      "metadata": {
        "id": "4GazjShWEa3Y"
      },
      "id": "4GazjShWEa3Y",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5b845035-b15e-4c17-af16-2fafc8f2c2ef",
      "metadata": {
        "id": "5b845035-b15e-4c17-af16-2fafc8f2c2ef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "from transformers import AdamW, BertModel, BertPreTrainedModel, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2bc9d89-1518-49d0-9060-e236f3664914",
      "metadata": {
        "id": "c2bc9d89-1518-49d0-9060-e236f3664914"
      },
      "source": [
        "### 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c62e6fe7-0c2d-4875-8ffd-a7505479a60a",
      "metadata": {
        "id": "c62e6fe7-0c2d-4875-8ffd-a7505479a60a"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"/content/train_data.csv\")\n",
        "test = pd.read_csv(\"/content/test_data.csv\")\n",
        "submission = pd.read_csv(\"/content/sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "_jylwuxsECft",
        "outputId": "5ff708cc-1dc9-4844-92c8-d8675f7a2c1b"
      },
      "id": "_jylwuxsECft",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                            premise  \\\n",
              "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
              "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
              "2      2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
              "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
              "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
              "\n",
              "                                hypothesis          label  \n",
              "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
              "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
              "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
              "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
              "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac642c9d-20f7-4b16-95e2-8b46096be383\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
              "      <td>씨름의 여자들의 놀이이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
              "      <td>자작극을 벌인 이는 3명이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
              "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
              "      <td>원주민들은 종합대책에 만족했다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
              "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac642c9d-20f7-4b16-95e2-8b46096be383')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac642c9d-20f7-4b16-95e2-8b46096be383 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac642c9d-20f7-4b16-95e2-8b46096be383');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas profiling"
      ],
      "metadata": {
        "id": "BRI180xiEPLj"
      },
      "id": "BRI180xiEPLj",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3af087d9-718a-4290-9d9f-cacbe20d0f9a",
      "metadata": {
        "id": "3af087d9-718a-4290-9d9f-cacbe20d0f9a"
      },
      "source": [
        "### 데이터셋 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d90f6e3f-d3a6-4db8-88c5-efa243a6f33e",
      "metadata": {
        "id": "d90f6e3f-d3a6-4db8-88c5-efa243a6f33e"
      },
      "outputs": [],
      "source": [
        "max_len = 70\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 5e-5\n",
        "\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model1**"
      ],
      "metadata": {
        "id": "4EEoTi73IIp8"
      },
      "id": "4EEoTi73IIp8"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3a396d1b-470b-4edd-97cf-1a2fc601980e",
      "metadata": {
        "id": "3a396d1b-470b-4edd-97cf-1a2fc601980e"
      },
      "outputs": [],
      "source": [
        "# bertmodel, vocab = get_pytorch_kobert_model(cachedir = \".cache\")\n",
        "# tokenizer = get_tokenizer()\n",
        "# tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model2**"
      ],
      "metadata": {
        "id": "WK4u7CYCIOlj"
      },
      "id": "WK4u7CYCIOlj"
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained('klue/roberta-base')\n",
        "config.num_labels = 3\n",
        "bertmodel = AutoModelForSequenceClassification.from_config(config)\n",
        "tok = AutoTokenizer.from_pretrained('klue/roberta-base')"
      ],
      "metadata": {
        "id": "n2ezKDT2IHdn"
      },
      "id": "n2ezKDT2IHdn",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "818k5hgqdr7J",
        "outputId": "c8d3e515-0eab-4432-c241-99ad998f26f8"
      },
      "id": "818k5hgqdr7J",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**"
      ],
      "metadata": {
        "id": "xS2FhIUHJ4_o"
      },
      "id": "xS2FhIUHJ4_o"
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}"
      ],
      "metadata": {
        "id": "13s4X5efK_Ek"
      },
      "id": "13s4X5efK_Ek",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"premise_\"] = \"[CLS]\" + train[\"premise\"] + \"[SEP]\"\n",
        "train[\"hypothesis_\"] = train[\"hypothesis\"] + \"[SEP]\"\n",
        "\n",
        "test[\"premise_\"] = \"[CLS]\" + test[\"premise\"] + \"[SEP]\"\n",
        "test[\"hypothesis_\"] = test[\"hypothesis\"] + \"[SEP]\"\n",
        "\n",
        "train[\"text_sum\"] = train.premise_ + \" \" + train.hypothesis_\n",
        "test[\"text_sum\"] = test.premise_ + \" \" + test.hypothesis_\n",
        "\n",
        "train_content = []\n",
        "test_content = []\n",
        "\n",
        "for i, text in enumerate(train.text_sum):\n",
        "    train_content.append(list([text, label_dict[train.label[i]]]))\n",
        "    \n",
        "for i, text in enumerate(test.text_sum):\n",
        "    test_content.append([text])\n",
        "    \n",
        "dataset_train = train_content[:20000]\n",
        "dataset_valid = train_content[20000:]\n",
        "dataset_test = test_content"
      ],
      "metadata": {
        "id": "bJq2r8GTK6D9"
      },
      "id": "bJq2r8GTK6D9",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_sentences = [row[0] for row in dataset_train]\n",
        "dataset_train_labels = [row[1] for row in dataset_train]\n",
        "\n",
        "dataset_valid_sentences = [row[0] for row in dataset_valid]\n",
        "dataset_valid_labels = [row[1] for row in dataset_valid]\n",
        "\n",
        "dataset_test_sentences = [row[0] for row in dataset_test]"
      ],
      "metadata": {
        "id": "hUjFrDb7LDAd"
      },
      "id": "hUjFrDb7LDAd",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_sentences = tok(\n",
        "    dataset_train_sentences,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False, # roberta = False\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=100,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "tokenized_valid_sentences = tok(\n",
        "    dataset_valid_sentences,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False, # roberta = False\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=100,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "tokenized_test_sentences = tok(\n",
        "    dataset_test_sentences,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False, # roberta = False\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=100,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "98LDMsKsJ4P_"
      },
      "id": "98LDMsKsJ4P_",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_sentences['input_ids'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpMPGK3HRV59",
        "outputId": "98eac5c0-249c-466f-bd34-1b8dddec17c8"
      },
      "id": "TpMPGK3HRV59",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,     0, 14441,  2073, 12382, 13169,  2200,  3797, 21505,  9005,\n",
              "         2259,  3997,  2031,  2079,  3661, 31221,  5845,  2200,  2112,    16,\n",
              "         5950, 15351, 17788,  7285,   748,  2088, 22048,  2470,  1132, 21893,\n",
              "        15351,  6481, 27135,  5417,  4084,  1972,  2145, 17524,  2138, 15526,\n",
              "         2259,   575, 28674,    18,     2, 14441,  2079,  3883,  2031,  2079,\n",
              "         5845, 28674,    18,     2,     2,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, tokenized_examples, labels=None):\n",
        "        self.tokenized_examples = tokenized_examples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        item = {key: value[i] for key, value in self.tokenized_examples.items()}\n",
        "        if self.labels:\n",
        "            item['labels'] = torch.tensor(self.labels[i])\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "metadata": {
        "id": "9pThhf5jNH6f"
      },
      "id": "9pThhf5jNH6f",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6d6ecf24-cc31-462f-a93d-6948d77b103c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d6ecf24-cc31-462f-a93d-6948d77b103c",
        "outputId": "0e34bac9-cb8c-4a73-ee07-fb1629286db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# create dataset\n",
        "data_train = BERTDataset(tokenized_train_sentences, dataset_train_labels)\n",
        "data_valid = BERTDataset(tokenized_valid_sentences, dataset_valid_labels)\n",
        "data_test = BERTDataset(tokenized_test_sentences)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 5)\n",
        "valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size = batch_size, num_workers = 5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeRqlAfeaFwO",
        "outputId": "ad35784b-b395-4426-8a1c-64a81a29f4ec"
      },
      "id": "yeRqlAfeaFwO",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0]),\n",
              " 'input_ids': tensor([    0,     0, 14441,  2073, 12382, 13169,  2200,  3797, 21505,  9005,\n",
              "          2259,  3997,  2031,  2079,  3661, 31221,  5845,  2200,  2112,    16,\n",
              "          5950, 15351, 17788,  7285,   748,  2088, 22048,  2470,  1132, 21893,\n",
              "         15351,  6481, 27135,  5417,  4084,  1972,  2145, 17524,  2138, 15526,\n",
              "          2259,   575, 28674,    18,     2, 14441,  2079,  3883,  2031,  2079,\n",
              "          5845, 28674,    18,     2,     2,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
              " 'labels': tensor(1)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-QWtcZRG9MS",
        "outputId": "9c1c80e4-caa5-4ca6-fb97-ea20cb5d2e6d"
      },
      "id": "y-QWtcZRG9MS",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "313"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b2f2712-51a1-44fd-814d-e170c5773cf7",
      "metadata": {
        "id": "7b2f2712-51a1-44fd-814d-e170c5773cf7"
      },
      "source": [
        "### 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "41442ea8-bce3-4cd5-b519-599aa0eea067",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41442ea8-bce3-4cd5-b519-599aa0eea067",
        "outputId": "b5075ad4-2e1f-4dcc-bef4-545b04b5441f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model = bertmodel.to(device)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a09bd67d-38c3-46d7-9378-b87c6fd4a60f",
      "metadata": {
        "id": "a09bd67d-38c3-46d7-9378-b87c6fd4a60f"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7596d75d-7339-4e68-a756-cd70f6669537",
      "metadata": {
        "id": "7596d75d-7339-4e68-a756-cd70f6669537"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "Fu21GJJstHpE"
      },
      "id": "Fu21GJJstHpE",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ef6ae5-4cae-4ac2-8cdf-1915448c30cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7ef6ae5-4cae-4ac2-8cdf-1915448c30cc",
        "outputId": "9a8ddb02-8c8f-4557-b4de-a72ff8b396ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 313/313 [03:09<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train acc 0.3350139776357827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:14<00:00,  5.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 valid acc 0.002109704641350211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 43%|████▎     | 136/313 [01:22<01:46,  1.66it/s]"
          ]
        }
      ],
      "source": [
        "# train and eval for each epochs\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    valid_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        token_ids = batch['input_ids'].to(device)\n",
        "        segment_ids = batch['attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "\n",
        "        # use amp\n",
        "        with autocast():\n",
        "            out = model(token_ids, segment_ids)\n",
        "            loss = loss_fn(out.logits, label) # caution => out.logits\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_acc += calc_accuracy(out.logits, label)\n",
        "\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model.eval()\n",
        "    # no gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for batch_id, batch in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
        "            token_ids = batch['input_ids'].to(device)\n",
        "            segment_ids = batch['attention_mask'].to(device)\n",
        "            label = batch['labels'].to(device)\n",
        "\n",
        "            out = model(token_ids, segment_ids)\n",
        "\n",
        "        valid_acc += calc_accuracy(out.logits, label)\n",
        "    print(\"epoch {} valid acc {}\".format(e+1, valid_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f79543d-b19c-483e-ad11-f9f10e0e55f2",
      "metadata": {
        "id": "9f79543d-b19c-483e-ad11-f9f10e0e55f2"
      },
      "source": [
        "### 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c487567c-1679-41bc-b272-6343b037083a",
      "metadata": {
        "id": "c487567c-1679-41bc-b272-6343b037083a"
      },
      "outputs": [],
      "source": [
        "# inference\n",
        "result = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_id, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
        "        token_ids = batch['input_ids'].to(device)\n",
        "        segment_ids = batch['attention_mask'].to(device)\n",
        "        out = model(token_ids, segment_ids)\n",
        "        result.append(out.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd386a25-6bf8-4598-9c89-2981c534538d",
      "metadata": {
        "id": "fd386a25-6bf8-4598-9c89-2981c534538d"
      },
      "outputs": [],
      "source": [
        "result_ = []\n",
        "for i in result:\n",
        "    for j in i:\n",
        "        result_.append(int(torch.argmax(j)))\n",
        "        \n",
        "out = [list(label_dict.keys())[_] for _ in result_]\n",
        "submission[\"label\"] = out\n",
        "submission.to_csv(\"sample_submission.csv\", index = False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "dacon-nli-competition-practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}